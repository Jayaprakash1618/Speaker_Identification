{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17AspBl04Ur4WuyuTERa2xe6lkJH9cSd_","timestamp":1692546144474}],"mount_file_id":"17AspBl04Ur4WuyuTERa2xe6lkJH9cSd_","authorship_tag":"ABX9TyP7+wwUVHK6iRLppnzoPHdH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sq3ic2hUH6aW","executionInfo":{"status":"ok","timestamp":1691560808057,"user_tz":-330,"elapsed":22127,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"39158e64-95e5-47ce-945b-c9264f1ed0ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","from os.path import isfile, join\n","import numpy as np\n","import shutil\n","from tensorflow import keras\n","from pathlib import Path\n","from IPython.display import display, Audio\n","import subprocess"],"metadata":{"id":"THwhxGTGIdpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_directory = \"/content/drive/MyDrive/16000_pcm_speeches\"\n","audio_folder = \"audio\"\n","noise_folder = \"noise\"\n","\n","audio_path = os.path.join(data_directory, audio_folder)\n","noise_path = os.path.join(data_directory, noise_folder)"],"metadata":{"id":"qJxW_0H2Ig0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["audio_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BSCnOvjIIgwz","executionInfo":{"status":"ok","timestamp":1691560849631,"user_tz":-330,"elapsed":11,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"13add235-35b4-4939-b33f-2db749648364"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/16000_pcm_speeches/audio'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["valid_split = 0.1\n","\n","shuffle_seed = 43\n","\n","sample_rate = 16000\n","\n","scale = 0.5\n","\n","batch_size = 128\n","\n","epochs = 15"],"metadata":{"id":"spxlQphFIguJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for folder in os.listdir(data_directory):\n","    if os.path.isdir(os.path.join(data_directory, folder)):\n","        if folder in [audio_folder, noise_folder]:\n","\n","            continue\n","        elif folder in [\"other\", \"_background_noise_\"]:\n","\n","            shutil.move(\n","                os.path.join(data_directory, folder),\n","                os.path.join(noise_path, folder),\n","            )\n","        else:\n","            shutil.move(\n","                os.path.join(data_directory, folder),\n","                os.path.join(audio_path, folder),\n","            )"],"metadata":{"id":"CWAsK4zgIgrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noise_paths = []\n","for subdir in os.listdir(noise_path):\n","    subdir_path = Path(noise_path) / subdir\n","    if os.path.isdir(subdir_path):\n","        noise_paths += [\n","            os.path.join(subdir_path, filepath)\n","            for filepath in os.listdir(subdir_path)\n","            if filepath.endswith(\".wav\")\n","        ]"],"metadata":{"id":"c8H5_jdwIgop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noise_paths"],"metadata":{"id":"pFl7LqfuIghV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691560924899,"user_tz":-330,"elapsed":732,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"306c3df8-7560-4766-c51d-e72fa034efab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/running_tap.wav',\n"," '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/10convert.com_Audience-Claps_daSG5fwdA7o.wav',\n"," '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/dude_miaowing.wav',\n"," '/content/drive/MyDrive/16000_pcm_speeches/noise/_background_noise_/doing_the_dishes.wav',\n"," '/content/drive/MyDrive/16000_pcm_speeches/noise/other/exercise_bike.wav',\n"," '/content/drive/MyDrive/16000_pcm_speeches/noise/other/pink_noise.wav']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["command = (\n","    \"for dir in `ls -1 \" + noise_path + \"`; do \"\n","    \"for file in `ls -1 \" + noise_path + \"/$dir/*.wav`; do \"\n","    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n","    \"$file | grep sample_rate | cut -f2 -d=`; \"\n","    \"if [ $sample_rate -ne 16000 ]; then \"\n","    \"ffmpeg -hide_banner -loglevel panic -y \"\n","    \"-i $file -ar 16000 temp.wav; \"\n","    \"mv temp.wav $file; \"\n","    \"fi; done; done\"\n",")"],"metadata":{"id":"6OEG76_VIgUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.system(command)\n","def load_noise_sample(path):\n","    sample, sampling_rate = tf.audio.decode_wav(\n","        tf.io.read_file(path), desired_channels=1\n","    )\n","    if sampling_rate == sample_rate:\n","        slices = int(sample.shape[0] / sample_rate)\n","        sample = tf.split(sample[: slices * sample_rate], slices)\n","        return sample\n","    else:\n","        print(\"Sampling rate for\",path, \"is incorrect\")\n","        return None\n","\n","\n","noises = []\n","for path in noise_paths:\n","    sample = load_noise_sample(path)\n","    if sample:\n","        noises.extend(sample)\n","noises = tf.stack(noises)"],"metadata":{"id":"S3cL_lFDIeR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def paths_and_labels_to_dataset(audio_paths, labels):\n","    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n","    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n","    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n","    return tf.data.Dataset.zip((audio_ds, label_ds))"],"metadata":{"id":"NfnuSmpHIeyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def path_to_audio(path):\n","    audio = tf.io.read_file(path)\n","    audio, _ = tf.audio.decode_wav(audio, 1, sample_rate)\n","    return audio"],"metadata":{"id":"OnIhmj8CIfSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_noise(audio, noises=None, scale=0.5):\n","    if noises is not None:\n","        tf_rnd = tf.random.uniform(\n","            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n","        )\n","        noise = tf.gather(noises, tf_rnd, axis=0)\n","\n","        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n","        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n","\n","        audio = audio + noise * prop * scale\n","\n","    return audio"],"metadata":{"id":"lno15OWY5Ap9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def audio_to_fft(audio):\n","    audio = tf.squeeze(audio, axis=-1)\n","    fft = tf.signal.fft(\n","        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n","    )\n","    fft = tf.expand_dims(fft, axis=-1)\n","\n","    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])"],"metadata":{"id":"xHIBBjWQ5FU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names = os.listdir(audio_path)\n","print(class_names,)\n","\n","audio_paths = []\n","labels = []\n","for label, name in enumerate(class_names):\n","    print(\"Speaker:\",(name))\n","    dir_path = Path(audio_path) / name\n","    speaker_sample_paths = [\n","        os.path.join(dir_path, filepath)\n","        for filepath in os.listdir(dir_path)\n","        if filepath.endswith(\".wav\")\n","    ]\n","    audio_paths += speaker_sample_paths\n","    labels += [label] * len(speaker_sample_paths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LClZXZWa5FOs","executionInfo":{"status":"ok","timestamp":1691561020007,"user_tz":-330,"elapsed":616,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"481f9e81-b40f-46b4-8eb4-f87c0a63f311"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela', 'Jens_Stoltenberg', 'Benjamin_Netanyau']\n","Speaker: Julia_Gillard\n","Speaker: Magaret_Tarcher\n","Speaker: Nelson_Mandela\n","Speaker: Jens_Stoltenberg\n","Speaker: Benjamin_Netanyau\n"]}]},{"cell_type":"code","source":["# Shuffle to generate random data\n","rng = np.random.RandomState(shuffle_seed)\n","rng.shuffle(audio_paths)\n","rng = np.random.RandomState(shuffle_seed)\n","rng.shuffle(labels)"],"metadata":{"id":"pfHQOU7l5E_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split into training and validation\n","num_val_samples = int(valid_split * len(audio_paths))\n","train_audio_paths = audio_paths[:-num_val_samples]\n","train_labels = labels[:-num_val_samples]\n","\n","\n","valid_audio_paths = audio_paths[-num_val_samples:]\n","valid_labels = labels[-num_val_samples:]"],"metadata":{"id":"uJMBaosH5QXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets, one for training and the other for validation\n","train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n","train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n","    batch_size\n",")\n","\n","valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n","valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)"],"metadata":{"id":"8y3YB98e5QMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add noise to the training set\n","train_ds = train_ds.map(\n","    lambda x, y: (add_noise(x, noises, scale=scale), y),\n","    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",")\n","\n","# Transform audio wave to the frequency domain using `audio_to_fft`\n","train_ds = train_ds.map(\n","    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",")\n","\n","train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","valid_ds = valid_ds.map(\n","    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",")\n","valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"],"metadata":{"id":"3O1QWsVm5Wv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Conv1D"],"metadata":{"id":"gNoR5I-E5WkG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def residual_block(x, filters, conv_num = 3, activation = \"relu\"):\n","    s = keras.layers.Conv1D(filters, 1, padding = \"same\")(x)\n","\n","    for i in range(conv_num - 1):\n","        x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n","        x = keras.layers.Activation(activation)(x)\n","\n","    x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n","    x = keras.layers.Add()([x, s])\n","    x = keras.layers.Activation(activation)(x)\n","\n","    return keras.layers.MaxPool1D(pool_size = 2, strides = 2)(x)\n","\n","def build_model(input_shape, num_classes):\n","    inputs = keras.layers.Input(shape = input_shape, name = \"input\")\n","\n","    x = residual_block(inputs, 16, 2)\n","    x = residual_block(inputs, 32, 2)\n","    x = residual_block(inputs, 64, 3)\n","    x = residual_block(inputs, 128, 3)\n","    x = residual_block(inputs, 128, 3)\n","    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n","    x = keras.layers.Flatten()(x)\n","    x = keras.layers.Dense(256, activation=\"relu\")(x)\n","    x = keras.layers.Dense(128, activation=\"relu\")(x)\n","\n","    outputs = keras.layers.Dense(num_classes, activation = \"softmax\", name = \"output\")(x)\n","\n","    return keras.models.Model(inputs = inputs, outputs = outputs)\n","\n","model = build_model((sample_rate // 2, 1), len(class_names))\n","\n","model.summary()\n","\n","model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","model_save_filename = \"model.h5\"\n","\n","earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","\n","mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vg7tQ2So5gk1","executionInfo":{"status":"ok","timestamp":1691561125108,"user_tz":-330,"elapsed":1230,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"4fd6c587-052e-4398-9230-9ac39d9bad1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input (InputLayer)             [(None, 8000, 1)]    0           []                               \n","                                                                                                  \n"," conv1d_15 (Conv1D)             (None, 8000, 128)    512         ['input[0][0]']                  \n","                                                                                                  \n"," activation_10 (Activation)     (None, 8000, 128)    0           ['conv1d_15[0][0]']              \n","                                                                                                  \n"," conv1d_16 (Conv1D)             (None, 8000, 128)    49280       ['activation_10[0][0]']          \n","                                                                                                  \n"," activation_11 (Activation)     (None, 8000, 128)    0           ['conv1d_16[0][0]']              \n","                                                                                                  \n"," conv1d_17 (Conv1D)             (None, 8000, 128)    49280       ['activation_11[0][0]']          \n","                                                                                                  \n"," conv1d_14 (Conv1D)             (None, 8000, 128)    256         ['input[0][0]']                  \n","                                                                                                  \n"," add_4 (Add)                    (None, 8000, 128)    0           ['conv1d_17[0][0]',              \n","                                                                  'conv1d_14[0][0]']              \n","                                                                                                  \n"," activation_12 (Activation)     (None, 8000, 128)    0           ['add_4[0][0]']                  \n","                                                                                                  \n"," max_pooling1d_4 (MaxPooling1D)  (None, 4000, 128)   0           ['activation_12[0][0]']          \n","                                                                                                  \n"," average_pooling1d (AveragePool  (None, 1333, 128)   0           ['max_pooling1d_4[0][0]']        \n"," ing1D)                                                                                           \n","                                                                                                  \n"," flatten (Flatten)              (None, 170624)       0           ['average_pooling1d[0][0]']      \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          43680000    ['flatten[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n","                                                                                                  \n"," output (Dense)                 (None, 5)            645         ['dense_1[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 43,812,869\n","Trainable params: 43,812,869\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["import os\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Define the path to save the model checkpoints\n","checkpoint_dir = '/content/drive/MyDrive/speaker_recognition_checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","checkpoint_path = os.path.join(checkpoint_dir, 'speaker_recognition_model.h5')\n","\n","# Check if there are any existing checkpoints and load the last one if available\n","if os.path.exists(checkpoint_path):\n","    model.load_weights(checkpoint_path)\n","    print(\"Resuming training from the last checkpoint.\")\n","\n","# Define the EarlyStopping callback\n","early_stopping_cb = EarlyStopping(patience=3, restore_best_weights=True)\n","\n","# Define the ModelCheckpoint callback\n","mdl_checkpoint_cb = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max')\n","\n","# Train the model for one epoch\n","history = model.fit(\n","    train_ds,\n","    epochs= 1,\n","    validation_data=valid_ds,\n","    callbacks=[early_stopping_cb, mdl_checkpoint_cb],\n",")\n","\n","# If you want to train for more epochs, you can do it like this:\n","# last_epoch = history.epoch[-1] + 1\n","# history = model.fit(\n","#     train_ds,\n","#     initial_epoch=last_epoch,\n","#     epochs=last_epoch + num_epochs,\n","#     validation_data=valid_ds,\n","#     callbacks=[early_stopping_cb, mdl_checkpoint_cb],\n","# )\n","\n","# Save the final model after training is complete (optional)\n","final_model_path = '/content/drive/MyDrive/speaker_recognition_final_model_2.h5'\n","model.save(final_model_path)\n","print(\"Model training is complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfGNdOQ95pzh","executionInfo":{"status":"ok","timestamp":1691563562414,"user_tz":-330,"elapsed":716216,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"04863a54-fe32-4dd1-9f87-baeb0b90e3eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Resuming training from the last checkpoint.\n","53/53 [==============================] - 713s 13s/step - loss: 0.1820 - accuracy: 0.9364 - val_loss: 0.1147 - val_accuracy: 0.9601\n","Model training is complete.\n"]}]},{"cell_type":"code","source":["saved_model_path = '/content/drive/MyDrive/speaker_recognition_final_model_2.h5'\n","\n","# Load the model\n","loaded_model = tf.keras.models.load_model(saved_model_path)"],"metadata":{"id":"5IntSXtW5rNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of model:\",model.evaluate(valid_ds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BemhPN1W7zDP","executionInfo":{"status":"ok","timestamp":1691563671268,"user_tz":-330,"elapsed":41489,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"2f06c8a8-ea49-49fa-eff0-d1c53f998bcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 24s 968ms/step - loss: 0.1147 - accuracy: 0.9601\n","Accuracy of model: [0.11468920111656189, 0.9600532650947571]\n"]}]},{"cell_type":"code","source":["SAMPLES_TO_DISPLAY = 10\n","\n","test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n","test_ds = test_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n","    batch_size\n",")\n","\n","test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n","\n","for audios, labels in test_ds.take(1):\n","    ffts = audio_to_fft(audios)\n","    y_pred = loaded_model.predict(ffts)\n","    rnd = np.random.randint(0, batch_size, SAMPLES_TO_DISPLAY)\n","    audios = audios.numpy()[rnd, :, :]\n","    labels = labels.numpy()[rnd]\n","    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n","\n","    for index in range(SAMPLES_TO_DISPLAY):\n","        print(\n","            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n","                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n","                class_names[labels[index]],\n","                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n","                class_names[y_pred[index]],\n","            )\n","        )\n","        if labels[index] ==y_pred[index]:\n","            print(\"Welcome\")\n","        else:\n","            print(\"Sorry\")\n","        print(\"The speaker is\" if labels[index] == y_pred[index] else \"\", class_names[y_pred[index]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DOPiGEJ72Dl","executionInfo":{"status":"ok","timestamp":1691563692442,"user_tz":-330,"elapsed":5300,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"4c870524-e329-4374-f628-7a6322244939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 3s 843ms/step\n","Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n","Welcome\n","The speaker is Julia_Gillard\n","Speaker:\u001b[92m Julia_Gillard\u001b[0m\tPredicted:\u001b[92m Julia_Gillard\u001b[0m\n","Welcome\n","The speaker is Julia_Gillard\n","Speaker:\u001b[91m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[91m Julia_Gillard\u001b[0m\n","Sorry\n"," Julia_Gillard\n","Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n","Welcome\n","The speaker is Benjamin_Netanyau\n","Speaker:\u001b[92m Magaret_Tarcher\u001b[0m\tPredicted:\u001b[92m Magaret_Tarcher\u001b[0m\n","Welcome\n","The speaker is Magaret_Tarcher\n","Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n","Welcome\n","The speaker is Jens_Stoltenberg\n","Speaker:\u001b[92m Benjamin_Netanyau\u001b[0m\tPredicted:\u001b[92m Benjamin_Netanyau\u001b[0m\n","Welcome\n","The speaker is Benjamin_Netanyau\n","Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n","Welcome\n","The speaker is Jens_Stoltenberg\n","Speaker:\u001b[92m Jens_Stoltenberg\u001b[0m\tPredicted:\u001b[92m Jens_Stoltenberg\u001b[0m\n","Welcome\n","The speaker is Jens_Stoltenberg\n","Speaker:\u001b[92m Magaret_Tarcher\u001b[0m\tPredicted:\u001b[92m Magaret_Tarcher\u001b[0m\n","Welcome\n","The speaker is Magaret_Tarcher\n"]}]},{"cell_type":"code","source":["def paths_to_dataset(audio_paths):\n","    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n","    return tf.data.Dataset.zip((path_ds))\n","\n","def predict(path, labels):\n","    test = paths_and_labels_to_dataset(path, labels)\n","\n","\n","    test = test.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n","    batch_size\n","    )\n","    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","\n","    test = test.map(lambda x, y: (add_noise(x, noises, scale=scale), y))\n","\n","    for audios, labels in test.take(1):\n","            ffts = audio_to_fft(audios)\n","            y_pred = model.predict(ffts)\n","            rnd = np.random.randint(0, 1, 1)\n","            audios = audios.numpy()[rnd, :]\n","            labels = labels.numpy()[rnd]\n","            y_pred = np.argmax(y_pred, axis=-1)[rnd]\n","\n","    for index in range(1):\n","            print(\n","            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n","            \"[92m\",y_pred[index],\n","                \"[92m\", y_pred[index]\n","                )\n","            )\n","\n","            print(\"Speaker Predicted:\",class_names[y_pred[index]])"],"metadata":{"id":"NfHZSX0_76qX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here in the path you have to give the path of the audio file that you want to predict .\n","\n","path = [\"/content/drive/MyDrive/16000_pcm_speeches/audio/Jens_Stoltenberg/1077.wav\"]\n","labels = [\"unknown\"]\n","try:\n","    predict(path, labels)\n","except:\n","    print(\" Error! Check if the file correctly passed or not! \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTPKmXIw7-2U","executionInfo":{"status":"ok","timestamp":1691563713221,"user_tz":-330,"elapsed":826,"user":{"displayName":"Jayaprakash p","userId":"15196863282532808905"}},"outputId":"2d305b10-821f-486a-8650-913218cad8b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 129ms/step\n","Speaker:\u001b[92m 3\u001b[0m\tPredicted:\u001b[92m 3\u001b[0m\n","Speaker Predicted: Jens_Stoltenberg\n"]}]}]}